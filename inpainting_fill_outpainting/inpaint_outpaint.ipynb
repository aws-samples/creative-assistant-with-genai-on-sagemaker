{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62e10f13-c7bf-463b-9648-1b9c7f4cccb6",
   "metadata": {},
   "source": [
    "## Creative Content Assisted by Generative AI using Amazon SageMaker: Inpainting Fill and Outpainting\n",
    "\n",
    "The first example is called **inpainting fill**. It is the process of replacing a portion of an image with synthesized content based on a textual prompt. We will accomplish this using Stable Diffusion XL(SDXL) model from Amazon Bedrock.\n",
    "\n",
    "The workflow is to provide the model with three inputs:\n",
    "\n",
    "- A mask image that outlines the portion to be replaced\n",
    "- A textual prompt describing the desired contents\n",
    "- The original image\n",
    "\n",
    "Bedrock can then produce a new image that replaces the masked area with the object, subject, or environment described in the prompt.\n",
    "\n",
    "You can follow this example [here](../inpainting_eraser/) to host a Segment Anything Model (SAM) and generate the masks, but we also provided a [mask image](data/mask.png) you can use out of the box for this example.\n",
    "\n",
    "**Note:** The mask image must have the same resolution and aspect ratio as the image being inpainted upon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c281bab",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422dd129",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uq sagemaker\n",
    "!pip install -Uq diffusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1195d0b7-9eff-4564-bed8-aea5199b36c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Built-Ins:\n",
    "import base64\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "from diffusers.utils import load_image, make_image_grid\n",
    "\n",
    "bedrock_runtime = boto3.client(service_name=\"bedrock-runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9026a088-2aa7-403e-b6d4-39efcb8c357f",
   "metadata": {},
   "source": [
    "We will send the image to Bedrock API in base64 encoding, so first let's prepare that. Here is the function that will convert a Pillow image to base64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b8a07e-a794-4eac-873f-fbac8cb6db84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_base64(img) -> str:\n",
    "    \"\"\"Convert a PIL Image or local image file path to a base64 string for Amazon Bedrock\"\"\"\n",
    "    if isinstance(img, str):\n",
    "        if os.path.isfile(img):\n",
    "            with open(img, \"rb\") as f:\n",
    "                return base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"File {img} does not exist\")\n",
    "    elif isinstance(img, Image.Image):\n",
    "        buffer = io.BytesIO()\n",
    "        img.save(buffer, format=\"PNG\")\n",
    "        return base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
    "    else:\n",
    "        raise ValueError(f\"Expected str (filename) or PIL Image. Got {type(img)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58756575",
   "metadata": {},
   "source": [
    "### Download the initial image & mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bfd400",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = load_image(\n",
    "    \"https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png\"\n",
    ")\n",
    "mask = Image.open('data/mask.jpg')\n",
    "\n",
    "make_image_grid([image, mask], rows=1, cols=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3bc8bf",
   "metadata": {},
   "source": [
    "### Use Stable Diffusion XL 1.0 from Bedrock to inpaint the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189a6450-5b9c-46c6-ac27-1d3de1124874",
   "metadata": {},
   "outputs": [],
   "source": [
    "inpaint_prompt = \"The Mona Lisa wearing a wig\"\n",
    "style_preset = \"digital-art\"  # (e.g. photographic, digital-art, cinematic, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7a3b83-90a2-487f-acfb-1793f69dadec",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "request = json.dumps({\n",
    "    \"text_prompts\":[{\"text\": inpaint_prompt}],\n",
    "    \"init_image\": image_to_base64(image),\n",
    "    \"mask_source\": \"MASK_IMAGE_WHITE\",\n",
    "    \"mask_image\": image_to_base64(mask),\n",
    "    \"cfg_scale\": 10,\n",
    "    \"seed\": 10,\n",
    "    \"style_preset\": style_preset,\n",
    "})\n",
    "modelId = \"stability.stable-diffusion-xl\"\n",
    "\n",
    "response = bedrock_runtime.invoke_model(body=request, modelId=modelId)\n",
    "response_body = json.loads(response.get(\"body\").read())\n",
    "\n",
    "image_2_b64_str = response_body[\"artifacts\"][0].get(\"base64\")\n",
    "inpaint = Image.open(io.BytesIO(base64.decodebytes(bytes(image_2_b64_str, \"utf-8\"))))\n",
    "\n",
    "make_image_grid([image, inpaint], rows=1, cols=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4de45c8",
   "metadata": {},
   "source": [
    "The second example is called **outpainting**. It is the process technique that extends or extrapolates beyond the original image borders. We will accomplish this using Titan Image Generator from Amazon Bedrock.\n",
    "\n",
    "The workflow is to provide the model with three inputs:\n",
    "\n",
    "- Extending the canvas of orginal image\n",
    "- creating an mask of extended area\n",
    "- A textual prompt describing the desired contents\n",
    "\n",
    "Titan Image generator can fill the extending area according to the textual prompt. This is very useful in situation will you need to change aspect ratio of the image, expanding repetitive textures, or expanding the scope of a scene by filling additional space and objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d0747c",
   "metadata": {},
   "source": [
    "### Extending the original image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d82266",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "original_width, original_height = image.size\n",
    "\n",
    "target_width = 1024 #extended canvas size\n",
    "target_height = 1024\n",
    "position = ( #position the existing image in the center of the larger canvas\n",
    "    int((target_width - original_width) * 0.5), \n",
    "    int((target_height - original_height) * 0.5),\n",
    ")\n",
    "\n",
    "extended_image = Image.new(\"RGB\", (target_width, target_height), (235, 235, 235))\n",
    "extended_image.paste(image, position)\n",
    "\n",
    "# create a mask of the extended area\n",
    "inside_color_value = (0, 0, 0) #inside is black - this is the masked area\n",
    "outside_color_value = (255, 255, 255)\n",
    "\n",
    "mask_image = Image.new(\"RGB\", (target_width, target_height), outside_color_value)\n",
    "original_image_shape = Image.new(\n",
    "    \"RGB\", (original_width-40, original_height-40), inside_color_value\n",
    ")\n",
    "mask_image.paste(original_image_shape, tuple(x+20 for x in position))\n",
    "make_image_grid([extended_image, mask_image], rows=1, cols=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed3fabc",
   "metadata": {},
   "source": [
    "## Use Titan Image Generator from Bedrock to extend the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f700cb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the inference parameters.\n",
    "request = json.dumps({\n",
    "    \"taskType\": \"OUTPAINTING\",\n",
    "    \"outPaintingParams\": {\n",
    "        \"image\": image_to_base64(extended_image),\n",
    "        \"maskImage\": image_to_base64(mask_image),\n",
    "        \"text\": \"A girl standing on a grass field in a dark night with stars and a full moon.\",  # Description of the background to generate\n",
    "        \"outPaintingMode\": \"DEFAULT\",  # \"DEFAULT\" softens the mask. \"PRECISE\" keeps it sharp.\n",
    "    },\n",
    "    \"imageGenerationConfig\": {\n",
    "        \"numberOfImages\": 1,  # Number of variations to generate\n",
    "        \"quality\": \"premium\",  # Allowed values are \"standard\" or \"premium\"\n",
    "        \"width\": target_width,\n",
    "        \"height\": target_height,\n",
    "        \"cfgScale\": 8,\n",
    "        \"seed\": 5763,  # Use a random seed\n",
    "    },\n",
    "})\n",
    "\n",
    "modelId = \"amazon.titan-image-generator-v1\"\n",
    "\n",
    "response = bedrock_runtime.invoke_model(body=request, modelId=modelId)\n",
    "response_body = json.loads(response.get(\"body\").read())\n",
    "\n",
    "image_bytes = base64.b64decode(response_body[\"images\"][0])\n",
    "outpaint = Image.open(io.BytesIO(image_bytes))\n",
    "\n",
    "make_image_grid([extended_image, outpaint], rows=1, cols=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee33d008",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
