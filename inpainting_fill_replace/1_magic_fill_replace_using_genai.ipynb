{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c33deb5",
   "metadata": {},
   "source": [
    "## Creative Content Assisted by Generative AI using Amazon SageMaker: Magic Fill/Replace\n",
    "---\n",
    "In this notebook, we will extend the inpainting eraser example by replacing the LaMa model with **[Stable Diffusion (SD) Inpaint](https://huggingface.co/stabilityai/stable-diffusion-2-inpainting)**. This example not only can erase object from image, it also can fill object or replace background with a simple text prompt. The diagram below illustrated the capabilities.\n",
    "\n",
    "![magic_fill_replace](https://raw.github.com/geekyutao/Inpaint-Anything/main/example/MainFramework.png)\n",
    "\n",
    "To generate segmentation, we used a foundation model developed by Meta Research called **[Segment Anything Model (SAM)](https://segment-anything.com/) - Apache-2.0 license**. This model is trained on a massive dataset called SA-1B with over 11 million images and 1.1 billion segmentation masks.  This massive scale gave Sam model unprecedented ability to identify and isolate objects from an image out of the box without training.\n",
    "\n",
    "To fill/replace, we will use **[Stable Diffusion (SD) Inpaint](https://huggingface.co/stabilityai/stable-diffusion-2-inpainting) - CreativeML Open RAIL++-M License** model from Stabilityai. This model does image-to-image, but also allows you to supply a mask. The model take the entire image as context and generate the mask region according to a set of text prompts.\n",
    "\n",
    "**Note: please run the `0_setup.ipynb` notebook first before starting on this example. We recommend to use pytorch kernel on SageMaker Notebook Instance using `ml.g4dn.xlarge`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2362bd34",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161e0b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker, boto3, json\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import io\n",
    "import base64\n",
    "\n",
    "import time\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "%matplotlib inline  \n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "sm_client = boto3.client(service_name=\"sagemaker\")\n",
    "runtime_sm_client = boto3.client(\"sagemaker-runtime\")\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "sagemaker_session = sagemaker.Session(boto_session=boto3.Session())\n",
    "region = sagemaker_session.boto_region_name\n",
    "account = sagemaker_session.account_id()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'magic-fill-replace'\n",
    "\n",
    "%store -r extended_triton_image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95ae564",
   "metadata": {},
   "source": [
    "## Serve models wtih Triton inference server\n",
    "\n",
    "We will use Triton Python backend to deploy and host these models on SageMaker MME. Triton server requires our models to be package in following folder structure. We can find these already provided in the `model_repo` folder.\n",
    "```\n",
    "|-model_repo\n",
    "    |---sam\n",
    "        |----1\n",
    "             |--model.py\n",
    "        |----config.pbtxt\n",
    "    |---sd_inpaint\n",
    "        |----1\n",
    "             |--model.py\n",
    "             |--mask_processing.py\n",
    "        |----config.pbtxt\n",
    "```\n",
    "\n",
    "We are using Python backend to load our models. In order to use Python backend, you will need at least a Triton config file, and a Python file named `model.py` that is the entry point for your model. Let's explore the structure for each file. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca564cbe",
   "metadata": {},
   "source": [
    "`config.pbtxt` is a manditory configuration file for Triton that config the backend type, batch size, input, output format, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccc9073",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cat model_repo/sd_inpaint/config.pbtxt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ab8cc4",
   "metadata": {},
   "source": [
    "Python backend script needs to define a TritonPythonModel class with four potential functions. Refer to [Triton Python backend documentation](https://github.com/triton-inference-server/python_backend) for more details\n",
    "\n",
    "```python\n",
    "import triton_python_backend_utils as pb_utils\n",
    "class TritonPythonModel:\n",
    "    \"\"\"Your Python model must use the same class name. Every Python model\n",
    "    that is created must have \"TritonPythonModel\" as the class name.\n",
    "    \"\"\"\n",
    "    def auto_complete_config(auto_complete_model_config):\n",
    "    def initialize(self, args):\n",
    "    def execute(self, requests):\n",
    "    def finalize(self):\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1900ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat model_repo/sd_inpaint/1/model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fc985e",
   "metadata": {},
   "source": [
    "## Deploy Models to MME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffa803b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf `find -type d -name .ipynb_checkpoints`  \n",
    "!find . | grep -E \"(__pycache__|\\.pyc$)\" | xargs sudo rm -rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d926c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"model_repo\"\n",
    "models = [\"sam\", \"sd_inpaint\"]\n",
    "v_ = 0\n",
    "\n",
    "model_targets = dict()\n",
    "for m in models:\n",
    "    \n",
    "    tar_name = f\"{m}-v{v_}.tar.gz\"\n",
    "    model_targets[m] = tar_name\n",
    "\n",
    "    !tar -C $model_dir -zcvf $tar_name $m\n",
    "    \n",
    "    sagemaker_session.upload_data(path=tar_name, key_prefix=f\"{prefix}/models\")\n",
    "\n",
    "    \n",
    "print(model_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4494caa5",
   "metadata": {},
   "source": [
    "**Define the Serving Container**\n",
    "\n",
    "Start with a container definition. Define the ModelDataUrl to specify the S3 directory that contains all the models that SageMaker multi-model endpoint will use to load and serve predictions. Set Mode to MultiModel to indicates SageMaker would create the endpoint with MME container specifications. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bca353",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data_url = f\"s3://{bucket}/{prefix}/models/\"\n",
    "ts = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "container = {\n",
    "    \"Image\": extended_triton_image_uri,\n",
    "    \"ModelDataUrl\": model_data_url,\n",
    "    \"Mode\": \"MultiModel\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23a036f",
   "metadata": {},
   "source": [
    "\n",
    "**Setup SM Model**\n",
    "\n",
    "Using the SageMaker boto3 client, create the model using [create_model](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_model) API. We will pass the container definition to the create model API along with ModelName and ExecutionRoleArn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7693e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_model_name = f\"{prefix}-models-{ts}\"\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=sm_model_name, ExecutionRoleArn=role, PrimaryContainer=container\n",
    ")\n",
    "\n",
    "print(\"Model Arn: \" + create_model_response[\"ModelArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78635c28",
   "metadata": {},
   "source": [
    "**Create a SageMaker endpoint configuration.**\n",
    "\n",
    "Create a multi-model endpoint configuration using create_endpoint_config boto3 API. Specify an accelerated GPU computing instance in InstanceType (we will use the same instance type that we are using to host our SageMaker Notebook). We recommend configuring your endpoints with at least two instances with real-life use-cases. This allows SageMaker to provide a highly available set of predictions across multiple Availability Zones for the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e381e269",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_name = f\"{prefix}-config-{ts}\"\n",
    "instance_type = 'ml.g5.2xlarge'\n",
    "\n",
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": instance_type,\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": sm_model_name,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d78814",
   "metadata": {},
   "source": [
    "**Create endpoint**\n",
    "\n",
    "Using the above endpoint configuration we create a new sagemaker endpoint and wait for the deployment to finish. The status will change to **InService** once the deployment is successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a7065d",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = f\"{prefix}-ep-{ts}\"\n",
    "\n",
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6162b9",
   "metadata": {},
   "source": [
    "## Invoking the models\n",
    "\n",
    "Now we can test our models. We want to firt call the sam model to generate a segmentation mask. \n",
    "\n",
    "---\n",
    "\n",
    "### Invoke SAM model\n",
    "This primary input for this model is the image and the [x, y] coordinates of the image pixle to locate the object. We need to encode the image into bytes before sending it to the endpoint.\n",
    "\n",
    "Optionally, you can also   pass in `point_labels` if you need segment object in multiple class. or `dilate_kernel_size` if you need to play with the sharpness of the mask. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b01969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(img):\n",
    "    \n",
    "    # Convert the image to bytes\n",
    "    with io.BytesIO() as output:\n",
    "        img.save(output, format=\"JPEG\")\n",
    "        img_bytes = output.getvalue()\n",
    "    \n",
    "    return base64.b64encode(img_bytes).decode('utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4eadd5",
   "metadata": {},
   "source": [
    "Here is how you can invoke the SageMaker MME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5d9db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pixle coordinate of dog in dog.jpg is 200, 450\n",
    "# pixle coordinate of dog in sample1.png is 750, 500\n",
    "img_file='statics/sample1.png'\n",
    "original_image = Image.open(img_file)\n",
    "\n",
    "print(\"Original Image\")\n",
    "display(original_image)\n",
    "original_image_bytes = encode_image(original_image)\n",
    "\n",
    "gen_args = json.dumps(dict(point_coords=[750, 500], point_labels=1, dilate_kernel_size=15))\n",
    "\n",
    "inputs = dict(image=original_image_bytes,\n",
    "              gen_args = gen_args)\n",
    "\n",
    "payload = {\n",
    "    \"inputs\":\n",
    "        [{\"name\": name, \"shape\": [1,1], \"datatype\": \"BYTES\", \"data\": [data]} for name, data in inputs.items()]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c963a754",
   "metadata": {},
   "source": [
    "Notice when you invoke the model the first time, the latency is much higher due to cold start. Every subsequent calls will be much faster because the model is cached in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fc69aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "response = runtime_sm_client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        ContentType=\"application/octet-stream\",\n",
    "        Body=json.dumps(payload),\n",
    "        TargetModel=model_targets[\"sam\"], \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2fafe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = json.loads(response[\"Body\"].read().decode(\"utf8\"))[\"outputs\"]\n",
    "mask_decoded = io.BytesIO(base64.b64decode(output[0][\"data\"][0]))\n",
    "mask_rgb = Image.open(mask_decoded).convert(\"RGB\")\n",
    "\n",
    "print(\"Object Mask\")\n",
    "display(mask_rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3499e3",
   "metadata": {},
   "source": [
    "### Invoke Stable Diffusion Inpaint Model\n",
    "\n",
    "we need to pass the `mask_rgb` which indicates which regions of the image should be filled. In parallel, you can use text prompt to control what to generate in the masked area. If you leave the prompt as an empty string, the model can provide the same effect as remove the object from the image. If you remove the black and white color of the mask, the model will replace the background instead of fill the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efeda77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs ==================\n",
    "# original_image_bytes\n",
    "mask_image = encode_image(mask_rgb)\n",
    "prompt = \"a teddy bear on a bench\"\n",
    "\n",
    "nprompt = \"ugly, distorted\"\n",
    "\n",
    "gen_args = json.dumps(dict(num_inference_steps=50, guidance_scale=10, seed=1))\n",
    "\n",
    "inputs = dict(image=original_image_bytes,\n",
    "              mask_image=mask_image,\n",
    "              prompt = prompt,\n",
    "              negative_prompt = nprompt,\n",
    "              gen_args = gen_args)\n",
    "\n",
    "payload = {\n",
    "    \"inputs\":\n",
    "        [{\"name\": name, \"shape\": [1,1], \"datatype\": \"BYTES\", \"data\": [data]} for name, data in inputs.items()]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84bfc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "response = runtime_sm_client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        ContentType=\"application/octet-stream\",\n",
    "        Body=json.dumps(payload),\n",
    "        TargetModel=model_targets[\"sd_inpaint\"], \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaab860e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = json.loads(response[\"Body\"].read().decode(\"utf8\"))[\"outputs\"]\n",
    "mask_decoded = io.BytesIO(base64.b64decode(output[0][\"data\"][0]))\n",
    "mask_rgb = Image.open(mask_decoded).convert(\"RGB\")\n",
    "\n",
    "print(\"Object Filled\")\n",
    "display(mask_rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b08aa1f",
   "metadata": {},
   "source": [
    "### [Optional] Gradio UI\n",
    "Write configuration file for the Gradio app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc8e467",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict()\n",
    "config[\"endpoint_name\"] = endpoint_name\n",
    "config[\"models\"] = model_targets\n",
    "\n",
    "with open(\"config.json\", 'w') as f:\n",
    "    json.dump(config, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2303f139",
   "metadata": {},
   "source": [
    "1) Open up a system terminal and navigate into this folder\n",
    "\n",
    "2) install packages. most important is to pip install `gradio`\n",
    "\n",
    "```\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "3) run the following commend to launch the app\n",
    "```\n",
    "python run.py\n",
    "```\n",
    "\n",
    "4) click on the public link to open up the ui in your browser."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8061c56a",
   "metadata": {},
   "source": [
    "### Clean Up\n",
    "When you are done delete the endpoint to stop incurring charges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8185d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sm_client.delete_endpoint(\n",
    "    EndpointName=endpoint_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1148052d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
